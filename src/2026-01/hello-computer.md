# Hello, Computer.

**Source**: https://spyglass.org/vocal-computing-ai/

**Date**: January 13, 2026

**Author**: M.G. Siegler

**Keywords**: vocal computing, voice interfaces, AI, LLMs, Siri, Alexa, wearables, smart assistants

## Elevator pitch

After nearly two decades of false starts, vocal computing may finally be on the verge of becoming a primary interface thanks to the breakthrough capabilities of large language models.

## Takeaways

- Voice computing has been "just around the corner" for almost 20 years, with Siri (2011), Alexa, and Google Assistant all failing to deliver on the revolutionary promise
- The fundamental missing ingredient was AI sophistication—early voice assistants relied on basic machine learning that couldn't handle complex, natural interactions
- Large language models, particularly following ChatGPT's emergence, have provided the technological foundation that voice interfaces always needed
- 2026 is shaping up as a pivotal year with OpenAI acquiring Jony Ive's hardware startup, new wearable form factors emerging, and major players reimagining their voice assistants
- The interface challenge remains: voice has been secondary within text-based chatbot UIs, limiting mainstream adoption despite improved underlying models

## Synthesis

M.G. Siegler's analysis of vocal computing arrives at a moment of genuine technological inflection. The piece is notable for its self-aware acknowledgment of the author's own history of premature predictions about voice interfaces—a humility that lends credibility to his argument that this time might actually be different.

The core thesis is straightforward but compelling: voice assistants failed not because the concept was flawed, but because the underlying AI wasn't sophisticated enough. Siri, launched in 2011 after Apple's acquisition, represented the first mainstream attempt at natural language interaction. Amazon's Alexa strategy and Google's entry into the space followed, yet all these systems relied on pattern matching and basic machine learning that broke down the moment users strayed from expected queries. The result was a generation of voice assistants that felt more like voice-activated search engines than actual assistants.

What changed, according to Siegler, is the advent of large language models. ChatGPT's launch marked a paradigm shift in what AI could understand and generate. For the first time, conversational AI could handle ambiguity, maintain context across exchanges, and produce responses that felt genuinely intelligent rather than mechanically retrieved from a database.

The piece identifies 2026 as a convergence year. Several developments are aligning: OpenAI's acquisition of the hardware startup founded by Jony Ive signals serious intent to build dedicated voice-first devices. New wearable form factors—pendants, clips, rings, and smart glasses—are emerging as alternatives to the smartphone screen. The major incumbents are reimagining their offerings, with Alexa+, Google Home, and Siri all receiving significant AI upgrades. Integration with existing hardware like AirPods creates pathways for adoption without requiring entirely new device purchases.

Yet Siegler identifies a crucial bottleneck: the interface problem. Despite dramatic improvements in model capability, voice has remained a secondary modality. Most users interact with AI through text-based chatbot interfaces, where typing feels natural and voice seems unnecessary. For vocal computing to truly arrive, it needs to become the primary interface rather than an optional add-on—and that requires purpose-built hardware and experiences designed around voice from the ground up.

The article ultimately suggests we're at the beginning of a genuine transition, not another false dawn. The technological foundations are finally in place. What remains is the harder work of building the interfaces, devices, and habits that will make speaking to computers as natural as speaking to humans.
